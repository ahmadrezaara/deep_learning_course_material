\section{Question 2}
In this exercise, we want to become more familiar with ML estimation and its relationship to VAE.
\subsection{Part a}
Suppose our dataset is 
\[
D = \{ x_1, x_2, \ldots, x_n \}.
\]
Study the concept of \emph{maximum likelihood} and explain why the parameters of the distribution 
must be chosen to maximize the following expression:
\[
\sum_{i=1}^{n} \log p_{\theta}(x_i).
\]
Note that \(p_{\theta}(x_i)\) indicates that the probability of seeing \(x_i\) in the output 
depends on the parameters \(\theta\).
\begin{qsolve}
    \begin{qsolve}[]
        Maximum Likelihood Estimation (MLE) finds the parameters \(\theta\) that make the dataset \(D = \{x_1, x_2, \ldots, x_n\}\) most likely. 

        The likelihood is:
        \[
        L(\theta) = \prod_{i=1}^{n} p_{\theta}(x_i),
        \]
        and we take the log to make it easier to work with:
        \[
        \log L(\theta) = \sum_{i=1}^{n} \log p_{\theta}(x_i).
        \]

        Maximizing \(\sum_{i=1}^{n} \log p_{\theta}(x_i)\) is the same as maximizing the likelihood, but simpler to compute. This helps us find the best \(\theta\) to explain the data.
    \end{qsolve}
\end{qsolve}
\subsection{Part b}
Show the equivalence between minimizing the cross-entropy error and performing ML estimation.
\begin{qsolve}
    \begin{qsolve}[]
        The cross-entropy error is defined as:
        \[
        \text{Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n} \log p_{\theta}(x_i).
        \]
        \splitqsolve[\splitqsolve]
        Maximizing the log-likelihood in ML estimation is:
        \[
        \log L(\theta) = \sum_{i=1}^{n} \log p_{\theta}(x_i).
        \]
        Minimizing the cross-entropy is equivalent to minimizing \(-\log L(\theta)\), which is the negative log-likelihood. Therefore, minimizing the cross-entropy error directly corresponds to performing Maximum Likelihood Estimation.    \end{qsolve}
\end{qsolve}
\subsection{Part c}
We can say that one of the goals of VAE is to have a \emph{generative model} whose output distribution 
resembles the distribution of the dataset. In a VAE---just like in standard neural networks---we intend to use 
a \emph{stochastic gradient descent (SGD)} algorithm. Typically, in ordinary neural networks, we would aim to maximize 
\(\sum_{i=1}^{n} \log p_{\theta}(x_i)\) for a mini-batch at each step. Then, for each input, we avoid making 
an overly drastic update to the network parameters; instead, we want to gradually increase the probability 
of generating an output similar to that input. Over the course of training, seeing that this log-probability 
increases is precisely in line with the concept of the \emph{ELBO} as well. Indeed:
\[
\log p_{\theta}(x_i)
- D_{\mathrm{KL}}\bigl[q_{\phi}(z \mid x_i) \,\|\, p_{\theta}(z \mid x_i)\bigr]
= \mathbb{E}_{z}\bigl[\log p_{\theta}(x_i \mid z)\bigr]
- D_{\mathrm{KL}}\bigl[q_{\phi}(z \mid x_i) \,\|\, p_{\theta}(z)\bigr].
\]

Here, \(\theta\) denotes the decoder parameters and \(\phi\) denotes the encoder parameters.

\textbf{(i)} Show that the KL divergence is nonnegative and explain how that fact arises in the above derivation.
\begin{qsolve}
    \begin{qsolve}[]
        The KL divergence is defined as:
        \[
        D_{\mathrm{KL}}(q(z) \,\|\, p(z)) = \int q(z) \log \frac{q(z)}{p(z)} \, dz.
        \]
        Rewriting it as:
        \[
        -D_{\mathrm{KL}}(q(z) \,\|\, p(z)) = -\int q(z) \log \frac{q(z)}{p(z)} \, dz.
        \]

        Using Jensen's inequality, the term inside the integral satisfies:
        \[
        -\int q(z) \log \frac{q(z)}{p(z)} \leq \log \int q(z) \frac{p(z)}{q(z)} \, dz.
        \]
        
        Simplifying the right-hand side:
        \[
        \log \int p(z) \, dz = \log 1 = 0.
        \]
        \splitqsolve[\splitqsolve]
        Thus:
        \[
        -D_{\mathrm{KL}}(q(z) \,\|\, p(z)) \leq 0,
        \]
        which implies \(D_{\mathrm{KL}}(q(z) \,\|\, p(z)) \geq 0\).
        
        This non-negativity in the derivation of the ELBO ensures that \(\log p_{\theta}(x_i)\) is greater than or equal to the ELBO:
        \[
        \log p_{\theta}(x_i) \geq \mathbb{E}_{z}\bigl[\log p_{\theta}(x_i \mid z)\bigr] - D_{\mathrm{KL}}\bigl[q_{\phi}(z \mid x_i) \,\|\, p_{\theta}(z)\bigr].
        \]
        Maximizing the ELBO indirectly maximizes \(\log p_{\theta}(x_i)\) by reducing the KL divergence and aligning \(q_{\phi}(z \mid x_i)\) with \(p_{\theta}(z \mid x_i)\), which ensures better generation and a tighter bound.
    \end{qsolve}
\end{qsolve}
\textbf{(ii)} Explain why, in various VAE implementations, the expression 
\(\mathbb{E}_{z}\bigl[\log p_{\theta}(x_i \mid z)\bigr]\) is commonly regarded as the cross-entropy loss measured between the distribution of the real data and the distribution generated by the decoder.
\begin{qsolve}
    \begin{qsolve}[]
        The term \(\mathbb{E}_{z}\bigl[\log p_{\theta}(x_i \mid z)\bigr]\) is the expected log-probability of the data point \(x_i\) under the decoderâ€™s predicted distribution \(p_{\theta}(x_i \mid z)\), where \(z\) is sampled from \(q_{\phi}(z \mid x_i)\).

        This term is regarded as the cross-entropy loss because it quantifies the difference between the true data distribution and the distribution generated by the decoder. Specifically, \(-\mathbb{E}_{z}[\log p_{\theta}(x_i \mid z)]\) corresponds to the cross-entropy loss, which combines both the reconstruction error (how well the decoder recreates \(x_i\)) and the match between distributions. By maximizing \(\mathbb{E}_{z}[\log p_{\theta}(x_i \mid z)]\), the decoder is encouraged to generate outputs that closely resemble the real data.
    \end{qsolve}
\end{qsolve}
