\section{Q3}

In the lesson, we became familiar with the U-Net structure. In this question, we aim to review the main features and training of this network. At the end, we analyze the functionality of Transposed Convolution. Below, the overall architecture of the network is provided for visualization. To gain a deeper understanding, it is recommended to study the related \href{https://arxiv.org/abs/1505.04597}{article}.
\subsection{part a}
In this network, the encoder and decoder are connected using Skip Connections. Explain the reason and impact of having these connections with respect to the article's content.
    
    \begin{qsolve}
      \begin{qsolve}[]
        In the U-Net architecture, skip connections link the encoder (contracting path) and decoder (expanding path) layers. The primary reason for these connections is to preserve high-resolution spatial features that are lost during the downsampling process. By transferring these fine-grained details to the decoder, the network ensures that both contextual information and precise localization are retained.

      The skip connections have a significant impact on U-Net's performance. They improve segmentation accuracy by helping the network recover fine spatial details, which is crucial for tasks requiring precise boundaries, such as biomedical image segmentation. Additionally, skip connections facilitate better gradient flow during backpropagation, enabling effective training of deep networks. The symmetric U-shaped architecture created by these connections allows the network to combine deep contextual features with high-resolution information seamlessly, leading to sharper and more accurate segmentation results.
      \end{qsolve}
    \end{qsolve}

\subsection{part b}
For training the network, the Random Deformation technique is used to increase the amount of training data. Based on the article, explain how this technique is implemented and its effect on the model's performance.
    
    \begin{qsolve}
      \begin{qsolve}[]
        Random deformation is used in U-Net to augment training data by applying smooth elastic transformations. This is achieved by displacing grid control points with random values sampled from a Gaussian distribution and interpolating the deformation smoothly across the image using bicubic interpolation.

This technique effectively simulates realistic variations, such as tissue deformations, which are common in biomedical images. As a result, the model becomes more robust to such transformations and generalizes better, even with limited annotated training data.

      \end{qsolve}
    \end{qsolve}

\subsection{part c}
Consider the matrices below. Using the specified filter and input, apply the operation of Transposed Convolution.
\[
\text{Input} = 
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
, \quad
\text{Filter} = 
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\]

\textbf{Note:} For part (c), provide a step-by-step explanation. You can use Python libraries such as PyTorch to check your answer.
    \begin{qsolve}
      \begin{qsolve}[]
        The input size is \( 2 \times 2 \), and the filter size is \( 2 \times 2 \). The output size will be:
      \[
      \text{Output size} = (H + K - 1) \times (W + K - 1) = (2 + 2 - 1) \times (2 + 2 - 1) = 3 \times 3.
      \]

      We start with a \( 3 \times 3 \) output matrix initialized to zeros:
      \[
      \text{Output} = 
      \begin{bmatrix}
      0 & 0 & 0 \\
      0 & 0 & 0 \\
      0 & 0 & 0
      \end{bmatrix}.
      \]


      Multiply \( 1 \) (top-left element of input) by the filter:
      \[
      1 \cdot
      \begin{bmatrix}
      1 & 2 \\
      3 & 4
      \end{bmatrix}
      =
      \begin{bmatrix}
      1 & 2 \\
      3 & 4
      \end{bmatrix}.
      \]
      Add this result to the top-left of the output matrix:
      \[
      \text{Output} =
      \begin{bmatrix}
      1 & 2 & 0 \\
      3 & 4 & 0 \\
      0 & 0 & 0
      \end{bmatrix}.
      \]


      Multiply \( 2 \) (top-right element of input) by the filter:
      \[
      2 \cdot
      \begin{bmatrix}
      1 & 2 \\
      3 & 4
      \end{bmatrix}
      =
      \begin{bmatrix}
      2 & 4 \\
      6 & 8
      \end{bmatrix}.
      \]
      Add this result to the output matrix starting from the top-right position:
      \[
      \text{Output} =
      \begin{bmatrix}
      1 & 4 & 4 \\
      3 & 10 & 8 \\
      0 & 0 & 0
      \end{bmatrix}.
      \]


      Multiply \( 3 \) (bottom-left element of input) by the filter:
      \[
      3 \cdot
      \begin{bmatrix}
      1 & 2 \\
      3 & 4
      \end{bmatrix}
      =
      \begin{bmatrix}
      3 & 6 \\
      9 & 12
      \end{bmatrix}.
      \]
      \splitqsolve[\splitqsolve]
      Add this result to the output matrix starting from the bottom-left position:
      \[
      \text{Output} =
      \begin{bmatrix}
      1 & 4 & 4 \\
      6 & 16 & 8 \\
      9 & 12 & 0
      \end{bmatrix}.
      \]

      Multiply \( 4 \) (bottom-right element of input) by the filter:
      \[
      4 \cdot
      \begin{bmatrix}
      1 & 2 \\
      3 & 4
      \end{bmatrix}
      =
      \begin{bmatrix}
      4 & 8 \\
      12 & 16
      \end{bmatrix}.
      \]
      Add this result to the output matrix starting from the bottom-right position:
      \[
      \text{Output} =
      \begin{bmatrix}
      1 & 4 & 4 \\
      6 & 20 & 16 \\
      9 & 24 & 16
      \end{bmatrix}.
      \]
      \end{qsolve}
      we can use this code to verify it:
      \begin{lstlisting}[language=Python]
import torch
import torch.nn.functional as F

# Define input and filter tensors
input_tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
filter_tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)

# Apply transposed convolution
output = F.conv_transpose2d(input_tensor, filter_tensor, stride=1, padding=0)
print("Output Matrix:")
print(output.squeeze(0).squeeze(0))
        \end{lstlisting}
        the output of this code is as follows:
        \begin{lstlisting}[language=Python]
        Output Matrix:
tensor([[ 1.,  4.,  4.],
        [ 6., 20., 16.],
        [ 9., 24., 16.]])
        \end{lstlisting}
    \end{qsolve}

